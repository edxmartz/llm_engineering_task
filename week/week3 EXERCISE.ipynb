{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Ejercicio adicional de fin de semana: semana 3\n",
    "\n",
    "Utlizando repositorios closed source (OpenAI, claude, gemini) u open source (HuggingFace), realizar:\n",
    "\n",
    "Utilizar modelos para generar datos sint√©ticos\n",
    "Utilizar una variedad de modelos y prompts para diversos resultados\n",
    "Crear una interfaz en Gradio UI para tu producto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57c4ae30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
      "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
      "  pip3 install [options] [-e] <vcs project url> ...\n",
      "  pip3 install [options] [-e] <local project path> ...\n",
      "  pip3 install [options] <archive url/path> ...\n",
      "\n",
      "no such option: --\n"
     ]
    }
   ],
   "source": [
    "!pip install -q-U bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import gradio as gr\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d46a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46be6e89600f4a30bf050d93b11f07c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# HuggingFace login\n",
    "\n",
    "# hf_token = userdata.get('HF_TOKEN')\n",
    "# login(hf_token, add_to_git_credential=True)\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4042dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI login\n",
    "api_key = getpass.getpass(\"Introduce tu OpenAI API Key y pulsa Enter: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "261f8b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Inicializaci√≥n\n",
    "# load_dotenv()\n",
    "\n",
    "# openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "# if openai_api_key:\n",
    "#     print(f\"OpenAI API Key configurada correctamente\")\n",
    "# else:\n",
    "#     print(\"OpenAI API Key sin configurar\")\n",
    "\n",
    "# Modelos\n",
    "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "OPENAI = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfcac5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization Config - Esto nos permite cargar el modelo en la memoria y utilizar menos memoria.\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9f9e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Eres un experto en ciencia de datos actuariales y normativa de seguros (GDPR/Solvencia II).\n",
    "Tu tarea es generar datos sint√©ticos realistas para pruebas de software.\n",
    "\n",
    "REGLAS CR√çTICAS:\n",
    "1. Formato: Devuelve √öNICAMENTE c√≥digo JSON puro. No incluyas explicaciones.\n",
    "2. Realismo: Las fechas de siniestro deben ser posteriores a la fecha de inicio de la p√≥liza.\n",
    "3. Coherencia: Si el cliente tiene un historial de alta siniestralidad, la prima (premium) debe ser proporcionalmente m√°s alta.\n",
    "4. Anonimizaci√≥n: Usa nombres ficticios y nunca generes n√∫meros de identificaci√≥n realistas (DNI/SSN).\n",
    "\"\"\"\n",
    "SYSTEM_PROMPT += \" El resultado debe ser una LISTA (array) de objetos JSON.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f79d7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76054e2369441bca7fb6639dc846690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9947f6f64c420e881a23298f43d4ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6350efa8d89429d9b82906a4f15c437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5267d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7a0bc00a7f449a98a7a59d202d5571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d7c814758e48e39c2f57f2c1ce8214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494f04de18584b5c88605873a56df8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f247f442104a4292c4545ea5130a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0dcd56fc7046428a9c27e44acccc94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096c45297e3b4579b039ec3ee8f16bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c96e3055fff4f5280d11539b8ebd472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2303f9d6325b4c37873637f8867065e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5aa3b2713eb4940a144b59005af32a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\",\n",
    "                                             quantization_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ae02742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_datos_seguros(perfil, cantidad, modelo_nombre):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"Genera exactamente {int(cantidad)} registros de tipo: {perfil}\"}\n",
    "    ]\n",
    "    \n",
    "    if modelo_nombre == LLAMA:\n",
    "        # 1. Aplicar el template del chat\n",
    "        inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        # 2. Generar con l√≠mites claros para evitar bucles infinitos\n",
    "        outputs = model.generate(\n",
    "            inputs, \n",
    "            max_new_tokens=2048, # Suficiente para varios JSONs\n",
    "            temperature=0.8,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # 3. Decodificar solo la parte nueva (la respuesta)\n",
    "        response_text = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
    "        return response_text\n",
    "    \n",
    "    else:\n",
    "        # L√≥gica para OpenAI\n",
    "        response = client.chat.completions.create(\n",
    "            model=modelo_nombre,\n",
    "            messages=messages,\n",
    "            temperature=0.8\n",
    "        )\n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddca393b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2107624074.py:1: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
      "  with gr.Blocks(theme=gr.themes.Soft()) as demo:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "* Running on public URL: https://87f95a6602ab689736.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://87f95a6602ab689736.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://87f95a6602ab689736.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# üõ°Ô∏è Actuarial Data Generator\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            perfil = gr.Dropdown([\"estandar\", \"fraude\", \"siniestro_vivienda\"], label=\"Perfil de Datos\")\n",
    "            cantidad = gr.Slider(1, 10, step=1, label=\"Cantidad de Registros\")\n",
    "            selector_modelo = gr.Radio([OPENAI, LLAMA], label=\"Modelo\", value=OPENAI)\n",
    "            btn = gr.Button(\"Generar JSON\")\n",
    "        \n",
    "        output_box = gr.Code(language=\"json\", label=\"Resultado\")\n",
    "\n",
    "    btn.click(\n",
    "        fn=generar_datos_seguros, \n",
    "        inputs=[perfil, cantidad, selector_modelo], \n",
    "        outputs=output_box\n",
    "    )\n",
    "\n",
    "# share=True es vital para ver la UI desde VS Code/Local\n",
    "demo.launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
